{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe1492ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sb = pd.read_csv('csv/scoreboard_2025.csv')\n",
    "bt = pd.read_csv('csv/batter_2025.csv')\n",
    "pt = pd.read_csv('csv/pitcher_2025.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f778e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜순 정렬\n",
    "sb = sb.sort_values(['year', 'month', 'day'])\n",
    "\n",
    "# [수정] 승률 계산을 위한 이진화 (1: 승리, 0: 패배/무승부)\n",
    "sb['win_binary'] = (sb['result'] == 1).astype(int)\n",
    "\n",
    "\n",
    "# 2. 타자 데이터를 경기(idx)별 팀 합계로 집계\n",
    "team_batting = bt.groupby('idx').agg({\n",
    "    'hit': 'sum',\n",
    "    'bat_num': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# 기본 경기 정보와 결합\n",
    "base_df = pd.merge(sb[['idx', 'team', 'year', 'month', 'day', 'dbheader', 'home', 'away', 'r', 'result', 'win_binary']], \n",
    "                   team_batting, on='idx', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b34f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. 상대 팀 득점(실점) 정보 매칭 (zfill 에러 수정)\n",
    "base_df['game_id'] = base_df['year'].astype(str) + \\\n",
    "                     base_df['month'].astype(str).str.zfill(2) + \\\n",
    "                     base_df['day'].astype(str).str.zfill(2) + \"_\" + \\\n",
    "                     base_df['dbheader'].astype(str) + \"_\" + \\\n",
    "                     base_df['home'] + \"_\" + base_df['away']\n",
    "\n",
    "\n",
    "opp_scores = base_df[['game_id', 'team', 'r']].rename(columns={'team': 'opp_team', 'r': 'runs_allowed'})\n",
    "base_df = pd.merge(base_df, opp_scores, on='game_id')\n",
    "base_df = base_df[base_df['team'] != base_df['opp_team']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ac9e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. [특성 1, 2, 3] 최근 30경기 이동 평균 (Rolling 30)\n",
    "base_df = base_df.sort_values(['team', 'year', 'month', 'day'])\n",
    "\n",
    "def get_rolling_features(group):\n",
    "    # ① 평균 득점, ② 평균 실점 (최근 30경기) [cite: 29, 30, 39]\n",
    "    group['f1_avg_runs_scored_30'] = group['r'].shift(1).rolling(window=30).mean()\n",
    "    group['f2_avg_runs_allowed_30'] = group['runs_allowed'].shift(1).rolling(window=30).mean()\n",
    "    \n",
    "    # ③ 팀 타율 (최근 30경기) [cite: 32, 39]\n",
    "    rolling_hits = group['hit'].shift(1).rolling(window=30).sum()\n",
    "    rolling_ab = group['bat_num'].shift(1).rolling(window=30).sum()\n",
    "    group['f3_team_batting_avg_30'] = rolling_hits / rolling_ab\n",
    "    return group\n",
    "\n",
    "base_df = base_df.groupby('team', group_keys=False).apply(get_rolling_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e9889a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           idx  year  month  day  dbheader home away   r  result  win_binary  \\\n",
      "0  20250308006  2025      3    8         0   롯데  KIA   3      -1           0   \n",
      "1  20250309006  2025      3    9         0   롯데  KIA   0       0           0   \n",
      "2  20250310006  2025      3   10         0   NC  KIA   3      -1           0   \n",
      "3  20250311006  2025      3   11         0   NC  KIA  17       1           1   \n",
      "4  20250313006  2025      3   13         0   두산  KIA   4       1           1   \n",
      "\n",
      "   hit  bat_num            game_id opp_team  runs_allowed  \\\n",
      "0    4       31  20250308_0_롯데_KIA       롯데             4   \n",
      "1    5       30  20250309_0_롯데_KIA       롯데             0   \n",
      "2    8       34  20250310_0_NC_KIA       NC             6   \n",
      "3   17       46  20250311_0_NC_KIA       NC            10   \n",
      "4    8       34  20250313_0_두산_KIA       두산             1   \n",
      "\n",
      "   f1_avg_runs_scored_30  f2_avg_runs_allowed_30  f3_team_batting_avg_30  \\\n",
      "0                    NaN                     NaN                     NaN   \n",
      "1                    NaN                     NaN                     NaN   \n",
      "2                    NaN                     NaN                     NaN   \n",
      "3                    NaN                     NaN                     NaN   \n",
      "4                    NaN                     NaN                     NaN   \n",
      "\n",
      "   f4_pitcher_runs_avg  \n",
      "0                  NaN  \n",
      "1                  NaN  \n",
      "2                  NaN  \n",
      "3                  NaN  \n",
      "4                  1.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. [특성 4] 선발 투수 시즌 평균 실점 [cite: 34]\n",
    "starters = pt[pt['mound'] == 1][['idx', 'name', 'losescore']]\n",
    "starters = pd.merge(starters, sb[['idx', 'team','year', 'month', 'day']], on='idx')\n",
    "starters = starters.sort_values(['name', 'year', 'month', 'day'])\n",
    "starters['f4_pitcher_runs_avg'] = starters.groupby('name')['losescore'].transform(lambda x: x.shift(1).expanding().mean())\n",
    "\n",
    "base_df = pd.merge(base_df, starters[['idx', 'f4_pitcher_runs_avg']], on='idx', how='left')\n",
    "print(base_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "267de178",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'team'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 6. [수정] 특성 5: 팀 전체 승률 (win_binary 사용) [cite: 36]\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m base_df[\u001b[33m'\u001b[39m\u001b[33mf5_total_win_pct\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mbase_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mteam\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mwin_binary\u001b[39m\u001b[33m'\u001b[39m].transform(\u001b[38;5;28;01mlambda\u001b[39;00m x: x.shift(\u001b[32m1\u001b[39m).expanding().mean())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pandas/util/_decorators.py:336\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    331\u001b[39m     warnings.warn(\n\u001b[32m    332\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    333\u001b[39m         klass,\n\u001b[32m    334\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    335\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py:10817\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m  10814\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m  10815\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m> \u001b[39m\u001b[32m10817\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10818\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10821\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10822\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10826\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1095\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1095\u001b[39m     grouper, exclusions, obj = \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[38;5;28mself\u001b[39m.observed = observed\n\u001b[32m   1105\u001b[39m \u001b[38;5;28mself\u001b[39m.obj = obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:901\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m    899\u001b[39m         in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    900\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    903\u001b[39m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m    904\u001b[39m     exclusions.add(gpr.key)\n",
      "\u001b[31mKeyError\u001b[39m: 'team'"
     ]
    }
   ],
   "source": [
    "# 6. [수정] 특성 5: 팀 전체 승률 (win_binary 사용) [cite: 36]\n",
    "base_df['f5_total_win_pct'] = base_df.groupby('team')['win_binary'].transform(lambda x: x.shift(1).expanding().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d51f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/my/9gb_8qfn3j52s7j_512g6fkw0000gn/T/ipykernel_17362/3522689615.py:10: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  base_df = base_df.groupby('team', group_keys=False).apply(get_ha_win_pct)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7. [수정] 특성 6: 홈/원정 승률 (win_binary 사용) [cite: 38]\n",
    "def get_ha_win_pct(group):\n",
    "    is_home = (group['team'] == group['home'])\n",
    "    group.loc[is_home, 'f6_ha_win_pct'] = group.loc[is_home, 'win_binary'].shift(1).expanding().mean()\n",
    "    \n",
    "    is_away = (group['team'] == group['away'])\n",
    "    group.loc[is_away, 'f6_ha_win_pct'] = group.loc[is_away, 'win_binary'].shift(1).expanding().mean()\n",
    "    return group\n",
    "\n",
    "base_df = base_df.groupby('team', group_keys=False).apply(get_ha_win_pct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5509ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. 최종 MLP 데이터셋 구성 (홈 6개 + 원정 6개 = 12개 입력값) [cite: 46]\n",
    "feature_cols = ['f1_avg_runs_scored_30', 'f2_avg_runs_allowed_30', 'f3_team_batting_avg_30', \n",
    "                'f4_pitcher_runs_avg', 'f5_total_win_pct', 'f6_ha_win_pct']\n",
    "\n",
    "home_data = base_df[base_df['team'] == base_df['home']][['game_id', 'result', 'win_binary'] + feature_cols]\n",
    "home_data.columns = ['game_id', 'raw_result', 'win_binary'] + ['h_' + c for c in feature_cols]\n",
    "\n",
    "away_data = base_df[base_df['team'] == base_df['away']][['game_id'] + feature_cols]\n",
    "away_data.columns = ['game_id'] + ['a_' + c for c in feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4742179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습용 데이터셋 생성 완료: kbo_mlp_training_data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 결측치 제거 후 병합\n",
    "final_dataset = pd.merge(home_data, away_data, on='game_id').dropna()\n",
    "\n",
    "# 무승부(0) 제거: 승/패만 학습에 사용\n",
    "final_dataset = final_dataset[final_dataset['raw_result'].isin([1, -1])].copy()\n",
    "\n",
    "# Target: 홈승이면 0, 원정승이면 1 (즉 홈이 졌으면 원정승)\n",
    "final_dataset['target'] = (final_dataset['raw_result'] == -1).astype(int)\n",
    "\n",
    "final_dataset = final_dataset.drop(columns=['raw_result', 'win_binary'])\n",
    "\n",
    "\n",
    "# 저장\n",
    "final_dataset.to_csv('kbo_mlp_training_data.csv', index=False)\n",
    "print(\"학습용 데이터셋 생성 완료: kbo_mlp_training_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7728fe",
   "metadata": {},
   "source": [
    "### skit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6464e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. 전처리된 데이터 로드\n",
    "df = pd.read_csv('kbo_mlp_training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba7e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. 특징(X)과 정답(y) 분리\n",
    "# game_id는 식별용이므로 제외, target(0:홈승, 1:원정승)을 예측\n",
    "X = df.drop(columns=['game_id', 'target'])\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. 데이터 분할 (훈련 75%, 테스트 25%) [cite: 20]\n",
    "# shuffle=False는 시간 순서대로 테스트하기 위함입니다 (자료 기준) [cite: 76]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 4. 데이터 스케일링 (StandardScaler 사용) [cite: 75, 77]\n",
    "sc_X = StandardScaler()\n",
    "X_train_scaled = sc_X.fit_transform(X_train)\n",
    "X_test_scaled = sc_X.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7bf9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 모델 학습 결과 ---\n",
      "Algorithm: SGD | Accuracy: 41.4%\n",
      "Algorithm: ADAM | Accuracy: 48.6%\n",
      "Algorithm: LBFGS | Accuracy: 52.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. 세 가지 최적화 알고리즘(Solver) 비교 [cite: 64, 71]\n",
    "solvers = ['sgd', 'adam', 'lbfgs']\n",
    "results = {}\n",
    "\n",
    "print(\"--- 모델 학습 결과 ---\")\n",
    "for s in solvers:\n",
    "    # 자료의 설정값 반영: 은닉층 (3,), 활성화 함수 relu, 최대 반복 1000 [cite: 47, 83]\n",
    "    clf = MLPClassifier(\n",
    "        hidden_layer_sizes=(12, 6), \n",
    "        activation='tanh', \n",
    "        solver=s, \n",
    "        max_iter=2000, \n",
    "        random_state=0\n",
    "    )\n",
    "    \n",
    "    # 모델 학습\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # 예측 및 정확도 계산 [cite: 85, 88]\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[s] = acc\n",
    "    print(f\"Algorithm: {s.upper()} | Accuracy: {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf22671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "추천 알고리즘: LBFGS (정확도 52.3%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 가장 높은 정확도를 보인 알고리즘 확인\n",
    "best_solver = max(results, key=results.get)\n",
    "print(f\"\\n추천 알고리즘: {best_solver.upper()} (정확도 {results[best_solver]:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea40e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:606: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최고 정확도: 54.1%\n",
      "최적의 파라미터: {'activation': 'tanh', 'hidden_layer_sizes': (12,), 'max_iter': 2000, 'solver': 'adam'}\n",
      "최종 테스트 결과: 43.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. 데이터 스케일링 (신경망에서 가장 중요!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. 테스트할 파라미터 조합 설정\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(3,), (6,), (12,), (6, 3), (12, 6)], # 노드 수를 늘려보거나 층을 쌓아봄\n",
    "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'max_iter': [2000] # 충분히 학습하도록 반복 횟수 증가\n",
    "}\n",
    "\n",
    "# 3. 그리드 서치 실행 (모든 조합을 다 해보고 최고를 찾음)\n",
    "mlp = MLPClassifier(random_state=1)\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. 결과 출력\n",
    "print(f\"최고 정확도: {grid_search.best_score_:.1%}\")\n",
    "print(f\"최적의 파라미터: {grid_search.best_params_}\")\n",
    "\n",
    "# 5. 최적의 모델로 테스트 데이터 평가\n",
    "best_model = grid_search.best_estimator_\n",
    "test_acc = best_model.score(X_test_scaled, y_test)\n",
    "print(f\"최종 테스트 결과: {test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0305f8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200] Loss: 0.6822 | Train Acc: 54.5% | Test Acc: 48.6%\n",
      "Epoch [40/200] Loss: 0.7039 | Train Acc: 56.1% | Test Acc: 45.9%\n",
      "Epoch [60/200] Loss: 0.6412 | Train Acc: 56.4% | Test Acc: 42.3%\n",
      "Epoch [80/200] Loss: 0.6789 | Train Acc: 56.8% | Test Acc: 43.2%\n",
      "Epoch [100/200] Loss: 0.7131 | Train Acc: 56.8% | Test Acc: 44.1%\n",
      "Epoch [120/200] Loss: 0.7371 | Train Acc: 56.8% | Test Acc: 43.2%\n",
      "Epoch [140/200] Loss: 0.6619 | Train Acc: 57.3% | Test Acc: 44.1%\n",
      "Epoch [160/200] Loss: 0.7288 | Train Acc: 55.9% | Test Acc: 42.3%\n",
      "Epoch [180/200] Loss: 0.7398 | Train Acc: 56.1% | Test Acc: 43.2%\n",
      "Epoch [200/200] Loss: 0.6697 | Train Acc: 56.1% | Test Acc: 42.3%\n",
      "\n",
      "[최종 테스트 정확도]: 42.34%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "df = pd.read_csv('kbo_mlp_training_data.csv')\n",
    "X = df.drop(columns=['game_id', 'target']).values\n",
    "y = df['target'].values.reshape(-1, 1)\n",
    "\n",
    "# 데이터 분할 (셔플 없이 시간 순서대로)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 텐서 변환\n",
    "X_train_t = torch.FloatTensor(X_train_scaled)\n",
    "y_train_t = torch.FloatTensor(y_train)\n",
    "X_test_t = torch.FloatTensor(X_test_scaled)\n",
    "y_test_t = torch.FloatTensor(y_test)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "# 2. MLP 모델 클래스 정의\n",
    "class KBOPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KBOPredictor, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(12, 12),  # 입력층 12 -> 은닉층 12\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(12, 6),   # 은닉층 12 -> 은닉층 6\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(6, 1),    # 은닉층 6 -> 출력층 1\n",
    "            nn.Sigmoid()        # 0~1 사이의 확률로 출력\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델, 손실함수, 최적화기 설정\n",
    "model = KBOPredictor()\n",
    "criterion = nn.BCELoss()\n",
    "# weight_decay는 가중치가 너무 커지지 않게 규제함 (L2 규제)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-2)\n",
    "\n",
    "# 3. 학습 루프\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 20회마다 평가 결과 출력\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_outputs = model(X_train_t)\n",
    "            train_acc = ((train_outputs > 0.5).float() == y_train_t).float().mean()\n",
    "            \n",
    "            test_outputs = model(X_test_t)\n",
    "            test_acc = ((test_outputs > 0.5).float() == y_test_t).float().mean()\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] Loss: {loss.item():.4f} | Train Acc: {train_acc:.1%} | Test Acc: {test_acc:.1%}\")\n",
    "\n",
    "# 4. 최종 결과 확인\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_pred = (model(X_test_t) > 0.5).float()\n",
    "    print(f\"\\n[최종 테스트 정확도]: {accuracy_score(y_test, final_pred):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eccb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200] Loss: 0.6595 | Train Acc: 56.4% | Test Acc: 47.7%\n",
      "Epoch [40/200] Loss: 0.6387 | Train Acc: 55.2% | Test Acc: 45.0%\n",
      "Epoch [60/200] Loss: 0.7135 | Train Acc: 56.6% | Test Acc: 43.2%\n",
      "Epoch [80/200] Loss: 0.6520 | Train Acc: 56.8% | Test Acc: 43.2%\n",
      "Epoch [100/200] Loss: 0.6689 | Train Acc: 57.0% | Test Acc: 41.4%\n",
      "Epoch [120/200] Loss: 0.6899 | Train Acc: 56.6% | Test Acc: 41.4%\n",
      "Epoch [140/200] Loss: 0.7022 | Train Acc: 56.8% | Test Acc: 42.3%\n",
      "Epoch [160/200] Loss: 0.7016 | Train Acc: 55.7% | Test Acc: 43.2%\n",
      "Epoch [180/200] Loss: 0.6525 | Train Acc: 56.4% | Test Acc: 40.5%\n",
      "Epoch [200/200] Loss: 0.6914 | Train Acc: 56.6% | Test Acc: 40.5%\n",
      "\n",
      "[최종 테스트 정확도]: 40.54%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "df = pd.read_csv('kbo_mlp_training_data.csv')\n",
    "X = df.drop(columns=['game_id', 'target']).values\n",
    "y = df['target'].values.reshape(-1, 1)\n",
    "\n",
    "# 데이터 분할 (셔플 없이 시간 순서대로)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 텐서 변환\n",
    "X_train_t = torch.FloatTensor(X_train_scaled)\n",
    "y_train_t = torch.FloatTensor(y_train)\n",
    "X_test_t = torch.FloatTensor(X_test_scaled)\n",
    "y_test_t = torch.FloatTensor(y_test)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=16, shuffle=True)\n",
    "\n",
    "# 2. 모델 설계 (Dropout 추가로 과적합 방지)\n",
    "class AdvancedKBOPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdvancedKBOPredictor, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(12, 8),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# 모델, 손실함수, 최적화기 설정\n",
    "model = AdvancedKBOPredictor()\n",
    "criterion = nn.BCELoss()\n",
    "# weight_decay는 가중치가 너무 커지지 않게 규제함 (L2 규제)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-2) # 1e-4 -> 1e-2\n",
    "\n",
    "# 3. 학습 루프\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 20회마다 평가 결과 출력\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_outputs = model(X_train_t)\n",
    "            train_acc = ((train_outputs > 0.5).float() == y_train_t).float().mean()\n",
    "            \n",
    "            test_outputs = model(X_test_t)\n",
    "            test_acc = ((test_outputs > 0.5).float() == y_test_t).float().mean()\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] Loss: {loss.item():.4f} | Train Acc: {train_acc:.1%} | Test Acc: {test_acc:.1%}\")\n",
    "\n",
    "# 4. 최종 결과 확인\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_pred = (model(X_test_t) > 0.5).float()\n",
    "    print(f\"\\n[최종 테스트 정확도]: {accuracy_score(y_test, final_pred):.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

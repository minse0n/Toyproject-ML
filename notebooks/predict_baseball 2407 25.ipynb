{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe1492ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sb = pd.read_csv('csv/scoreboard_2024_09_2025.csv')\n",
    "bt = pd.read_csv('csv/batter_2024_09_2025.csv')\n",
    "pt = pd.read_csv('csv/pitcher_2024_09_2025.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f778e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜순 정렬\n",
    "sb = sb.sort_values(['year', 'month', 'day'])\n",
    "\n",
    "# [수정] 승률 계산을 위한 이진화 (1: 승리, 0: 패배/무승부)\n",
    "sb['win_binary'] = (sb['result'] == 1).astype(int)\n",
    "\n",
    "\n",
    "# 2. 타자 데이터를 경기(idx)별 팀 합계로 집계\n",
    "team_batting = bt.groupby('idx').agg({\n",
    "    'hit': 'sum',\n",
    "    'bat_num': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# 기본 경기 정보와 결합\n",
    "base_df = pd.merge(sb[['idx', 'team', 'year', 'month', 'day', 'dbheader', 'home', 'away', 'r', 'result', 'win_binary']], \n",
    "                   team_batting, on='idx', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b34f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. 상대 팀 득점(실점) 정보 매칭 (zfill 에러 수정)\n",
    "base_df['game_id'] = base_df['year'].astype(str) + \\\n",
    "                     base_df['month'].astype(str).str.zfill(2) + \\\n",
    "                     base_df['day'].astype(str).str.zfill(2) + \"_\" + \\\n",
    "                     base_df['dbheader'].astype(str) + \"_\" + \\\n",
    "                     base_df['home'] + \"_\" + base_df['away']\n",
    "\n",
    "\n",
    "opp_scores = base_df[['game_id', 'team', 'r']].rename(columns={'team': 'opp_team', 'r': 'runs_allowed'})\n",
    "base_df = pd.merge(base_df, opp_scores, on='game_id')\n",
    "base_df = base_df[base_df['team'] != base_df['opp_team']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ac9e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. [특성 1, 2, 3] 최근 30경기 이동 평균 (Rolling 30)\n",
    "base_df = base_df.sort_values(['team', 'year', 'month', 'day'])\n",
    "\n",
    "def get_rolling_features(group):\n",
    "    # ① 평균 득점, ② 평균 실점 (최근 30경기) [cite: 29, 30, 39]\n",
    "    group['f1_avg_runs_scored_30'] = group['r'].shift(1).rolling(window=30).mean()\n",
    "    group['f2_avg_runs_allowed_30'] = group['runs_allowed'].shift(1).rolling(window=30).mean()\n",
    "    \n",
    "    # ③ 팀 타율 (최근 30경기) [cite: 32, 39]\n",
    "    rolling_hits = group['hit'].shift(1).rolling(window=30).sum()\n",
    "    rolling_ab = group['bat_num'].shift(1).rolling(window=30).sum()\n",
    "    group['f3_team_batting_avg_30'] = rolling_hits / rolling_ab\n",
    "    return group\n",
    "\n",
    "base_df = base_df.groupby('team', group_keys=False).apply(get_rolling_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e9889a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. [특성 4] 선발 투수 시즌 평균 실점 [cite: 34]\n",
    "starters = pt[pt['mound'] == 1][['idx', 'name', 'losescore']]\n",
    "starters = pd.merge(starters, sb[['idx', 'year', 'month', 'day']], on='idx')\n",
    "starters = starters.sort_values(['name', 'year', 'month', 'day'])\n",
    "starters['f4_pitcher_runs_avg'] = starters.groupby('name')['losescore'].transform(lambda x: x.shift(1).expanding().mean())\n",
    "\n",
    "base_df = pd.merge(base_df, starters[['idx', 'f4_pitcher_runs_avg']], on='idx', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "267de178",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'team'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 6. [수정] 특성 5: 팀 전체 승률 (win_binary 사용) [cite: 36]\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m base_df[\u001b[33m'\u001b[39m\u001b[33mf5_total_win_pct\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mbase_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mteam\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mwin_binary\u001b[39m\u001b[33m'\u001b[39m].transform(\u001b[38;5;28;01mlambda\u001b[39;00m x: x.shift(\u001b[32m1\u001b[39m).expanding().mean())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pandas/util/_decorators.py:336\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    331\u001b[39m     warnings.warn(\n\u001b[32m    332\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    333\u001b[39m         klass,\n\u001b[32m    334\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    335\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py:10817\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m  10814\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m  10815\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m> \u001b[39m\u001b[32m10817\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10818\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10821\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10822\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10826\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1095\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1095\u001b[39m     grouper, exclusions, obj = \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[38;5;28mself\u001b[39m.observed = observed\n\u001b[32m   1105\u001b[39m \u001b[38;5;28mself\u001b[39m.obj = obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:901\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m    899\u001b[39m         in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    900\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    903\u001b[39m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m    904\u001b[39m     exclusions.add(gpr.key)\n",
      "\u001b[31mKeyError\u001b[39m: 'team'"
     ]
    }
   ],
   "source": [
    "# 6. [수정] 특성 5: 팀 전체 승률 (win_binary 사용) [cite: 36]\n",
    "base_df['f5_total_win_pct'] = base_df.groupby('team')['win_binary'].transform(lambda x: x.shift(1).expanding().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d51f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. [수정] 특성 6: 홈/원정 승률 (win_binary 사용) [cite: 38]\n",
    "def get_ha_win_pct(group):\n",
    "    is_home = (group['team'] == group['home'])\n",
    "    group.loc[is_home, 'f6_ha_win_pct'] = group.loc[is_home, 'win_binary'].shift(1).expanding().mean()\n",
    "    \n",
    "    is_away = (group['team'] == group['away'])\n",
    "    group.loc[is_away, 'f6_ha_win_pct'] = group.loc[is_away, 'win_binary'].shift(1).expanding().mean()\n",
    "    return group\n",
    "\n",
    "base_df = base_df.groupby('team', group_keys=False).apply(get_ha_win_pct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5509ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'team'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:3641\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3640\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3642\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:168\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:197\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7668\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7676\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'team'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 8. 최종 MLP 데이터셋 구성 (홈 6개 + 원정 6개 = 12개 입력값) [cite: 46]\u001b[39;00m\n\u001b[32m      2\u001b[39m feature_cols = [\u001b[33m'\u001b[39m\u001b[33mf1_avg_runs_scored_30\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mf2_avg_runs_allowed_30\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mf3_team_batting_avg_30\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m      3\u001b[39m                 \u001b[33m'\u001b[39m\u001b[33mf4_pitcher_runs_avg\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mf5_total_win_pct\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mf6_ha_win_pct\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m home_data = base_df[\u001b[43mbase_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mteam\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m == base_df[\u001b[33m'\u001b[39m\u001b[33mhome\u001b[39m\u001b[33m'\u001b[39m]][[\u001b[33m'\u001b[39m\u001b[33mgame_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwin_binary\u001b[39m\u001b[33m'\u001b[39m] + feature_cols]\n\u001b[32m      6\u001b[39m home_data.columns = [\u001b[33m'\u001b[39m\u001b[33mgame_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mraw_result\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwin_binary\u001b[39m\u001b[33m'\u001b[39m] + [\u001b[33m'\u001b[39m\u001b[33mh_\u001b[39m\u001b[33m'\u001b[39m + c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m feature_cols]\n\u001b[32m      8\u001b[39m away_data = base_df[base_df[\u001b[33m'\u001b[39m\u001b[33mteam\u001b[39m\u001b[33m'\u001b[39m] == base_df[\u001b[33m'\u001b[39m\u001b[33maway\u001b[39m\u001b[33m'\u001b[39m]][[\u001b[33m'\u001b[39m\u001b[33mgame_id\u001b[39m\u001b[33m'\u001b[39m] + feature_cols]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py:4378\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4378\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4380\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:3648\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3644\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3645\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3646\u001b[39m     ):\n\u001b[32m   3647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3650\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3651\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3652\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3653\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'team'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. 최종 MLP 데이터셋 구성 (홈 6개 + 원정 6개 = 12개 입력값) [cite: 46]\n",
    "feature_cols = ['f1_avg_runs_scored_30', 'f2_avg_runs_allowed_30', 'f3_team_batting_avg_30', \n",
    "                'f4_pitcher_runs_avg', 'f5_total_win_pct', 'f6_ha_win_pct']\n",
    "\n",
    "home_data = base_df[base_df['team'] == base_df['home']][['game_id', 'result', 'win_binary'] + feature_cols]\n",
    "home_data.columns = ['game_id', 'raw_result', 'win_binary'] + ['h_' + c for c in feature_cols]\n",
    "\n",
    "away_data = base_df[base_df['team'] == base_df['away']][['game_id'] + feature_cols]\n",
    "away_data.columns = ['game_id'] + ['a_' + c for c in feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4742179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습용 데이터셋 생성 완료: kbo_mlp_training_data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 결측치 제거 후 병합\n",
    "final_dataset = pd.merge(home_data, away_data, on='game_id').dropna()\n",
    "\n",
    "# 무승부(0) 제거: 승/패만 학습에 사용\n",
    "final_dataset = final_dataset[final_dataset['raw_result'].isin([1, -1])].copy()\n",
    "\n",
    "# Target: 홈승이면 0, 원정승이면 1 (즉 홈이 졌으면 원정승)\n",
    "final_dataset['target'] = (final_dataset['raw_result'] == -1).astype(int)\n",
    "\n",
    "final_dataset = final_dataset.drop(columns=['raw_result', 'win_binary'])\n",
    "\n",
    "\n",
    "# 저장\n",
    "final_dataset.to_csv('kbo_mlp_training_data.csv', index=False)\n",
    "print(\"학습용 데이터셋 생성 완료: kbo_mlp_training_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7728fe",
   "metadata": {},
   "source": [
    "### skit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6464e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. 전처리된 데이터 로드\n",
    "df = pd.read_csv('kbo_mlp_training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba7e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. 특징(X)과 정답(y) 분리\n",
    "# game_id는 식별용이므로 제외, target(0:홈승, 1:원정승)을 예측\n",
    "X = df.drop(columns=['game_id', 'target'])\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776f210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.82923389 -0.13608825  2.3896831  ...  1.62582348  1.19067157\n",
      "   1.16990639]\n",
      " [ 1.31203772 -0.18055125  2.80207175 ...  0.46479099 -0.77519319\n",
      "  -0.42405794]\n",
      " [ 1.31203772 -0.5362552   2.94324749 ...  0.64070501 -0.93202873\n",
      "  -0.68336314]\n",
      " ...\n",
      " [ 1.02235542 -0.80303316  0.30121306 ... -1.82209118  0.95869219\n",
      "   1.0445071 ]\n",
      " [-1.53650486  0.79763462 -1.7183722  ...  2.04801712  0.35658323\n",
      "   1.06694698]\n",
      " [-1.48822448  1.24226456 -1.64698259 ...  0.02500596  0.33030606\n",
      "   1.30032166]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. 데이터 분할 (훈련 75%, 테스트 25%) [cite: 20]\n",
    "# shuffle=False는 시간 순서대로 테스트하기 위함입니다 (자료 기준) [cite: 76]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 4. 데이터 스케일링 (StandardScaler 사용) [cite: 75, 77]\n",
    "sc_X = StandardScaler()\n",
    "\n",
    "X_train_scaled = sc_X.fit_transform(X_train)\n",
    "X_test_scaled = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7bf9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 모델 학습 결과 ---\n",
      "Algorithm: SGD | Accuracy: 51.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: ADAM | Accuracy: 47.7%\n",
      "Algorithm: LBFGS | Accuracy: 47.7%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. 세 가지 최적화 알고리즘(Solver) 비교 [cite: 64, 71]\n",
    "solvers = ['sgd', 'adam', 'lbfgs']\n",
    "results = {}\n",
    "\n",
    "print(\"--- 모델 학습 결과 ---\")\n",
    "for s in solvers:\n",
    "    # 자료의 설정값 반영: 은닉층 (3,), 활성화 함수 relu, 최대 반복 1000 [cite: 47, 83]\n",
    "    clf = MLPClassifier(\n",
    "        hidden_layer_sizes=(12, 6), \n",
    "        activation='tanh', \n",
    "        solver=s, \n",
    "        max_iter=2000, \n",
    "        random_state=0\n",
    "    )\n",
    "    \n",
    "    # 모델 학습\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # 예측 및 정확도 계산 [cite: 85, 88]\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[s] = acc\n",
    "    print(f\"Algorithm: {s.upper()} | Accuracy: {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf22671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "추천 알고리즘: SGD (정확도 51.7%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 가장 높은 정확도를 보인 알고리즘 확인\n",
    "best_solver = max(results, key=results.get)\n",
    "print(f\"\\n추천 알고리즘: {best_solver.upper()} (정확도 {results[best_solver]:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea40e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:606: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:606: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최고 정확도: 55.3%\n",
      "최적의 파라미터: {'activation': 'tanh', 'hidden_layer_sizes': (12, 6), 'max_iter': 2000, 'solver': 'adam'}\n",
      "최종 테스트 결과: 45.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. 데이터 스케일링 (신경망에서 가장 중요!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. 테스트할 파라미터 조합 설정\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(3,), (6,), (12,), (6, 3), (12, 6)], # 노드 수를 늘려보거나 층을 쌓아봄\n",
    "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'max_iter': [2000] # 충분히 학습하도록 반복 횟수 증가\n",
    "}\n",
    "\n",
    "# 3. 그리드 서치 실행 (모든 조합을 다 해보고 최고를 찾음)\n",
    "mlp = MLPClassifier(random_state=1)\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. 결과 출력\n",
    "print(f\"최고 정확도: {grid_search.best_score_:.1%}\")\n",
    "print(f\"최적의 파라미터: {grid_search.best_params_}\")\n",
    "\n",
    "# 5. 최적의 모델로 테스트 데이터 평가\n",
    "best_model = grid_search.best_estimator_\n",
    "test_acc = best_model.score(X_test_scaled, y_test)\n",
    "print(f\"최종 테스트 결과: {test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0305f8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200] Loss: 0.6884 | Train Acc: 55.6% | Test Acc: 56.5%\n",
      "Epoch [40/200] Loss: 0.7183 | Train Acc: 56.5% | Test Acc: 51.6%\n",
      "Epoch [60/200] Loss: 0.6718 | Train Acc: 55.9% | Test Acc: 48.4%\n",
      "Epoch [80/200] Loss: 0.6661 | Train Acc: 55.8% | Test Acc: 54.8%\n",
      "Epoch [100/200] Loss: 0.6578 | Train Acc: 55.4% | Test Acc: 53.2%\n",
      "Epoch [120/200] Loss: 0.7007 | Train Acc: 55.8% | Test Acc: 58.1%\n",
      "Epoch [140/200] Loss: 0.7167 | Train Acc: 55.6% | Test Acc: 55.9%\n",
      "Epoch [160/200] Loss: 0.7078 | Train Acc: 55.0% | Test Acc: 58.1%\n",
      "Epoch [180/200] Loss: 0.7008 | Train Acc: 56.1% | Test Acc: 57.5%\n",
      "Epoch [200/200] Loss: 0.6688 | Train Acc: 55.8% | Test Acc: 58.6%\n",
      "\n",
      "[최종 테스트 정확도]: 58.60%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "df = pd.read_csv('kbo_mlp_training_data.csv')\n",
    "X = df.drop(columns=['game_id', 'target']).values\n",
    "y = df['target'].values.reshape(-1, 1)\n",
    "\n",
    "# 데이터 분할 (셔플 없이 시간 순서대로)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 텐서 변환\n",
    "X_train_t = torch.FloatTensor(X_train_scaled)\n",
    "y_train_t = torch.FloatTensor(y_train)\n",
    "X_test_t = torch.FloatTensor(X_test_scaled)\n",
    "y_test_t = torch.FloatTensor(y_test)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "# 2. MLP 모델 클래스 정의\n",
    "class KBOPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KBOPredictor, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(12, 12),  # 입력층 12 -> 은닉층 12\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(12, 6),   # 은닉층 12 -> 은닉층 6\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(6, 1),    # 은닉층 6 -> 출력층 1\n",
    "            nn.Sigmoid()        # 0~1 사이의 확률로 출력\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델, 손실함수, 최적화기 설정\n",
    "model = KBOPredictor()\n",
    "criterion = nn.BCELoss()\n",
    "# weight_decay는 가중치가 너무 커지지 않게 규제함 (L2 규제)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-2)\n",
    "\n",
    "# 3. 학습 루프\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 20회마다 평가 결과 출력\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_outputs = model(X_train_t)\n",
    "            train_acc = ((train_outputs > 0.5).float() == y_train_t).float().mean()\n",
    "            \n",
    "            test_outputs = model(X_test_t)\n",
    "            test_acc = ((test_outputs > 0.5).float() == y_test_t).float().mean()\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] Loss: {loss.item():.4f} | Train Acc: {train_acc:.1%} | Test Acc: {test_acc:.1%}\")\n",
    "\n",
    "# 4. 최종 결과 확인\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_pred = (model(X_test_t) > 0.5).float()\n",
    "    print(f\"\\n[최종 테스트 정확도]: {accuracy_score(y_test, final_pred):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eccb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200] Loss: 0.6216 | Train Acc: 53.5% | Test Acc: 47.7%\n",
      "Epoch [40/200] Loss: 0.7189 | Train Acc: 55.8% | Test Acc: 51.7%\n",
      "Epoch [60/200] Loss: 0.6734 | Train Acc: 54.1% | Test Acc: 51.7%\n",
      "Epoch [80/200] Loss: 0.7967 | Train Acc: 55.8% | Test Acc: 50.3%\n",
      "Epoch [100/200] Loss: 0.6995 | Train Acc: 55.6% | Test Acc: 47.0%\n",
      "Epoch [120/200] Loss: 0.5840 | Train Acc: 57.2% | Test Acc: 51.0%\n",
      "Epoch [140/200] Loss: 0.8945 | Train Acc: 56.3% | Test Acc: 51.0%\n",
      "Epoch [160/200] Loss: 0.6400 | Train Acc: 57.0% | Test Acc: 51.0%\n",
      "Epoch [180/200] Loss: 0.6716 | Train Acc: 56.0% | Test Acc: 54.4%\n",
      "Epoch [200/200] Loss: 0.6401 | Train Acc: 56.3% | Test Acc: 59.1%\n",
      "\n",
      "[최종 테스트 정확도]: 59.06%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "df = pd.read_csv('kbo_mlp_training_data.csv')\n",
    "X = df.drop(columns=['game_id', 'target']).values\n",
    "y = df['target'].values.reshape(-1, 1)\n",
    "\n",
    "# 데이터 분할 (셔플 없이 시간 순서대로)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 텐서 변환\n",
    "X_train_t = torch.FloatTensor(X_train_scaled)\n",
    "y_train_t = torch.FloatTensor(y_train)\n",
    "X_test_t = torch.FloatTensor(X_test_scaled)\n",
    "y_test_t = torch.FloatTensor(y_test)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=16, shuffle=True)\n",
    "\n",
    "# 2. 모델 설계 (Dropout 추가로 과적합 방지)\n",
    "class AdvancedKBOPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdvancedKBOPredictor, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(12, 8),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# 모델, 손실함수, 최적화기 설정\n",
    "model = AdvancedKBOPredictor()\n",
    "criterion = nn.BCELoss()\n",
    "# weight_decay는 가중치가 너무 커지지 않게 규제함 (L2 규제)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-2) # 1e-4 -> 1e-2\n",
    "\n",
    "# 3. 학습 루프\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 20회마다 평가 결과 출력\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_outputs = model(X_train_t)\n",
    "            train_acc = ((train_outputs > 0.5).float() == y_train_t).float().mean()\n",
    "            \n",
    "            test_outputs = model(X_test_t)\n",
    "            test_acc = ((test_outputs > 0.5).float() == y_test_t).float().mean()\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] Loss: {loss.item():.4f} | Train Acc: {train_acc:.1%} | Test Acc: {test_acc:.1%}\")\n",
    "\n",
    "# 4. 최종 결과 확인\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_pred = (model(X_test_t) > 0.5).float()\n",
    "    print(f\"\\n[최종 테스트 정확도]: {accuracy_score(y_test, final_pred):.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

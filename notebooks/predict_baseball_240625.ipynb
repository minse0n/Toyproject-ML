{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4742179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습용 데이터셋 생성 완료: 1085 경기 데이터 포함\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 로드\n",
    "sb = pd.read_csv('csv/scoreboard_2024_06_2025.csv')\n",
    "bt = pd.read_csv('csv/batter_2024_06_2025.csv')\n",
    "pt = pd.read_csv('csv/pitcher_2024_06_2025.csv')\n",
    "\n",
    "# 날짜순 정렬 및 승리 여부 이진화\n",
    "sb = sb.sort_values(['year', 'month', 'day', 'starttime'])\n",
    "sb['win_binary'] = (sb['result'] == 1).astype(int)\n",
    "\n",
    "# 2. 타자 데이터를 경기(idx)별 팀 합계로 집계\n",
    "team_batting = bt.groupby('idx').agg({\n",
    "    'hit': 'sum',\n",
    "    'bat_num': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# 기본 경기 정보와 결합\n",
    "base_df = pd.merge(sb[['idx', 'team', 'year', 'month', 'day', 'home', 'away', 'r', 'win_binary', 'dbheader']], \n",
    "                   team_batting, on='idx', how='left')\n",
    "\n",
    "# 3. 상대 팀 득점(실점) 정보 매칭 (더블헤더 dbheader 포함)\n",
    "# game_id에 dbheader를 추가하여 더블헤더 경기를 구분합니다.\n",
    "base_df['game_id'] = (base_df['year'].astype(str) + \n",
    "                     base_df['month'].astype(str).str.zfill(2) + \n",
    "                     base_df['day'].astype(str).str.zfill(2) + \"_\" + \n",
    "                     base_df['home'] + \"_\" + base_df['away'] + \"_\" + \n",
    "                     base_df['dbheader'].astype(str))\n",
    "\n",
    "opp_scores = base_df[['game_id', 'team', 'r']].rename(columns={'team': 'opp_team', 'r': 'runs_allowed'})\n",
    "base_df = pd.merge(base_df, opp_scores, on='game_id')\n",
    "base_df = base_df[base_df['team'] != base_df['opp_team']].copy()\n",
    "\n",
    "# 4. [특성 1, 2, 3] 최근 30경기 이동 평균 (transform 사용으로 컬럼 유지)\n",
    "# ※ 주의: 7월 데이터만 사용할 경우 30경기 미만이면 NaN이 발생하므로 min_periods를 설정합니다.\n",
    "base_df = base_df.sort_values(['team', 'year', 'month', 'day', 'game_id'])\n",
    "\n",
    "# 평균 득점 및 실점\n",
    "base_df['f1_avg_runs_scored_30'] = base_df.groupby('team')['r'].transform(lambda x: x.shift(1).rolling(window=30, min_periods=1).mean())\n",
    "base_df['f2_avg_runs_allowed_30'] = base_df.groupby('team')['runs_allowed'].transform(lambda x: x.shift(1).rolling(window=30, min_periods=1).mean())\n",
    "\n",
    "# 팀 타율 (누적 합계를 구한 뒤 나누는 방식이 더 정확합니다)\n",
    "rolling_hits = base_df.groupby('team')['hit'].transform(lambda x: x.shift(1).rolling(window=30, min_periods=1).sum())\n",
    "rolling_ab = base_df.groupby('team')['bat_num'].transform(lambda x: x.shift(1).rolling(window=30, min_periods=1).sum())\n",
    "base_df['f3_team_batting_avg_30'] = rolling_hits / rolling_ab\n",
    "\n",
    "# 5. [특성 4] 선발 투수 시즌 평균 실점\n",
    "starters = pt[pt['mound'] == 1][['idx', 'name', 'losescore']].copy()\n",
    "starters = pd.merge(starters, sb[['idx', 'year', 'month', 'day']], on='idx')\n",
    "starters = starters.sort_values(['name', 'year', 'month', 'day'])\n",
    "starters['f4_pitcher_runs_avg'] = starters.groupby('name')['losescore'].transform(lambda x: x.shift(1).expanding().mean())\n",
    "\n",
    "base_df = pd.merge(base_df, starters[['idx', 'f4_pitcher_runs_avg']], on='idx', how='left')\n",
    "\n",
    "# 6. [특성 5] 팀 전체 승률\n",
    "base_df['f5_total_win_pct'] = base_df.groupby('team')['win_binary'].transform(lambda x: x.shift(1).expanding().mean())\n",
    "\n",
    "# 7. [특성 6] 홈/원정 승률\n",
    "def calc_ha_win_pct(df):\n",
    "    df = df.copy()\n",
    "    df['is_home'] = (df['team'] == df['home'])\n",
    "    # 홈일 때와 원정일 때를 각각 그룹화하여 승률 계산\n",
    "    df['f6_ha_win_pct'] = df.groupby(['team', 'is_home'])['win_binary'].transform(lambda x: x.shift(1).expanding().mean())\n",
    "    return df\n",
    "\n",
    "base_df = calc_ha_win_pct(base_df)\n",
    "\n",
    "# 8. 최종 MLP 데이터셋 구성\n",
    "feature_cols = ['f1_avg_runs_scored_30', 'f2_avg_runs_allowed_30', 'f3_team_batting_avg_30', \n",
    "                'f4_pitcher_runs_avg', 'f5_total_win_pct', 'f6_ha_win_pct']\n",
    "\n",
    "# 홈팀 데이터와 원정팀 데이터 분리 후 결합\n",
    "home_df = base_df[base_df['team'] == base_df['home']][['game_id', 'win_binary'] + feature_cols]\n",
    "home_df.columns = ['game_id', 'home_win'] + ['h_' + c for c in feature_cols]\n",
    "\n",
    "away_df = base_df[base_df['team'] == base_df['away']][['game_id'] + feature_cols]\n",
    "away_df.columns = ['game_id'] + ['a_' + c for c in feature_cols]\n",
    "\n",
    "final_dataset = pd.merge(home_df, away_df, on='game_id').dropna()\n",
    "\n",
    "# Target: 홈팀 승리 시 0, 원정팀 승리 시 1\n",
    "final_dataset['target'] = (final_dataset['home_win'] == 0).astype(int)\n",
    "final_dataset = final_dataset.drop(columns=['home_win'])\n",
    "\n",
    "# 저장\n",
    "final_dataset.to_csv('kbo_mlp_training_data.csv', index=False)\n",
    "print(f\"학습용 데이터셋 생성 완료: {len(final_dataset)} 경기 데이터 포함\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7728fe",
   "metadata": {},
   "source": [
    "### skit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6464e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. 전처리된 데이터 로드\n",
    "df = pd.read_csv('kbo_mlp_training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fba7e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. 특징(X)과 정답(y) 분리\n",
    "# game_id는 식별용이므로 제외, target(0:홈승, 1:원정승)을 예측\n",
    "X = df.drop(columns=['game_id', 'target'])\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0776f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. 데이터 분할 (훈련 75%, 테스트 25%) [cite: 20]\n",
    "# shuffle=False는 시간 순서대로 테스트하기 위함입니다 (자료 기준) [cite: 76]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 4. 데이터 스케일링 (StandardScaler 사용) [cite: 75, 77]\n",
    "sc_X = StandardScaler()\n",
    "X_train_scaled = sc_X.fit_transform(X_train)\n",
    "X_test_scaled = sc_X.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec7bf9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 모델 학습 결과 ---\n",
      "Algorithm: SGD | Accuracy: 52.5%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: ADAM | Accuracy: 51.2%\n",
      "Algorithm: LBFGS | Accuracy: 51.6%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:606: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. 세 가지 최적화 알고리즘(Solver) 비교 [cite: 64, 71]\n",
    "solvers = ['sgd', 'adam', 'lbfgs']\n",
    "results = {}\n",
    "\n",
    "print(\"--- 모델 학습 결과 ---\")\n",
    "for s in solvers:\n",
    "    # 자료의 설정값 반영: 은닉층 (3,), 활성화 함수 relu, 최대 반복 1000 [cite: 47, 83]\n",
    "    clf = MLPClassifier(\n",
    "        hidden_layer_sizes=(12, 6), \n",
    "        activation='tanh', \n",
    "        solver=s, \n",
    "        max_iter=2000, \n",
    "        random_state=0\n",
    "    )\n",
    "    \n",
    "    # 모델 학습\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # 예측 및 정확도 계산 [cite: 85, 88]\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[s] = acc\n",
    "    print(f\"Algorithm: {s.upper()} | Accuracy: {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcf22671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "추천 알고리즘: SGD (정확도 52.5%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 가장 높은 정확도를 보인 알고리즘 확인\n",
    "best_solver = max(results, key=results.get)\n",
    "print(f\"\\n추천 알고리즘: {best_solver.upper()} (정확도 {results[best_solver]:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ea40e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:606: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:606: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:606: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:606: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:606: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:606: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:606: ConvergenceWarning: lbfgs failed to converge after 2000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=2000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최고 정확도: 53.7%\n",
      "최적의 파라미터: {'activation': 'tanh', 'hidden_layer_sizes': (6, 3), 'max_iter': 2000, 'solver': 'adam'}\n",
      "최종 테스트 결과: 48.8%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. 데이터 스케일링 (신경망에서 가장 중요!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 2. 테스트할 파라미터 조합 설정\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(3,), (6,), (12,), (6, 3), (12, 6)], # 노드 수를 늘려보거나 층을 쌓아봄\n",
    "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'max_iter': [2000] # 충분히 학습하도록 반복 횟수 증가\n",
    "}\n",
    "\n",
    "# 3. 그리드 서치 실행 (모든 조합을 다 해보고 최고를 찾음)\n",
    "mlp = MLPClassifier(random_state=1)\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. 결과 출력\n",
    "print(f\"최고 정확도: {grid_search.best_score_:.1%}\")\n",
    "print(f\"최적의 파라미터: {grid_search.best_params_}\")\n",
    "\n",
    "# 5. 최적의 모델로 테스트 데이터 평가\n",
    "best_model = grid_search.best_estimator_\n",
    "test_acc = best_model.score(X_test_scaled, y_test)\n",
    "print(f\"최종 테스트 결과: {test_acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0305f8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200] Loss: 0.7076 | Train Acc: 54.5% | Test Acc: 53.0%\n",
      "Epoch [40/200] Loss: 0.6962 | Train Acc: 54.3% | Test Acc: 54.4%\n",
      "Epoch [60/200] Loss: 0.6699 | Train Acc: 54.1% | Test Acc: 56.2%\n",
      "Epoch [80/200] Loss: 0.6980 | Train Acc: 55.1% | Test Acc: 55.3%\n",
      "Epoch [100/200] Loss: 0.6763 | Train Acc: 54.6% | Test Acc: 56.2%\n",
      "Epoch [120/200] Loss: 0.6990 | Train Acc: 55.0% | Test Acc: 55.3%\n",
      "Epoch [140/200] Loss: 0.7009 | Train Acc: 54.1% | Test Acc: 57.1%\n",
      "Epoch [160/200] Loss: 0.7230 | Train Acc: 55.1% | Test Acc: 55.8%\n",
      "Epoch [180/200] Loss: 0.6925 | Train Acc: 54.6% | Test Acc: 56.7%\n",
      "Epoch [200/200] Loss: 0.7214 | Train Acc: 53.5% | Test Acc: 56.7%\n",
      "\n",
      "[최종 테스트 정확도]: 56.68%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "df = pd.read_csv('kbo_mlp_training_data.csv')\n",
    "X = df.drop(columns=['game_id', 'target']).values\n",
    "y = df['target'].values.reshape(-1, 1)\n",
    "\n",
    "# 데이터 분할 (셔플 없이 시간 순서대로)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 텐서 변환\n",
    "X_train_t = torch.FloatTensor(X_train_scaled)\n",
    "y_train_t = torch.FloatTensor(y_train)\n",
    "X_test_t = torch.FloatTensor(X_test_scaled)\n",
    "y_test_t = torch.FloatTensor(y_test)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "# 2. MLP 모델 클래스 정의\n",
    "class KBOPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KBOPredictor, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(12, 12),  # 입력층 12 -> 은닉층 12\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(12, 6),   # 은닉층 12 -> 은닉층 6\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(6, 1),    # 은닉층 6 -> 출력층 1\n",
    "            nn.Sigmoid()        # 0~1 사이의 확률로 출력\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 모델, 손실함수, 최적화기 설정\n",
    "model = KBOPredictor()\n",
    "criterion = nn.BCELoss()\n",
    "# weight_decay는 가중치가 너무 커지지 않게 규제함 (L2 규제)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-2)\n",
    "\n",
    "# 3. 학습 루프\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 20회마다 평가 결과 출력\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_outputs = model(X_train_t)\n",
    "            train_acc = ((train_outputs > 0.5).float() == y_train_t).float().mean()\n",
    "            \n",
    "            test_outputs = model(X_test_t)\n",
    "            test_acc = ((test_outputs > 0.5).float() == y_test_t).float().mean()\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] Loss: {loss.item():.4f} | Train Acc: {train_acc:.1%} | Test Acc: {test_acc:.1%}\")\n",
    "\n",
    "# 4. 최종 결과 확인\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_pred = (model(X_test_t) > 0.5).float()\n",
    "    print(f\"\\n[최종 테스트 정확도]: {accuracy_score(y_test, final_pred):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15eccb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200] Loss: 0.6894 | Train Acc: 55.3% | Test Acc: 58.1%\n",
      "Epoch [40/200] Loss: 0.6759 | Train Acc: 54.5% | Test Acc: 54.8%\n",
      "Epoch [60/200] Loss: 0.6757 | Train Acc: 54.1% | Test Acc: 56.2%\n",
      "Epoch [80/200] Loss: 0.6524 | Train Acc: 54.7% | Test Acc: 56.7%\n",
      "Epoch [100/200] Loss: 0.7317 | Train Acc: 54.7% | Test Acc: 55.3%\n",
      "Epoch [120/200] Loss: 0.6924 | Train Acc: 54.7% | Test Acc: 55.8%\n",
      "Epoch [140/200] Loss: 0.7032 | Train Acc: 54.5% | Test Acc: 55.8%\n",
      "Epoch [160/200] Loss: 0.6845 | Train Acc: 55.0% | Test Acc: 55.8%\n",
      "Epoch [180/200] Loss: 0.6884 | Train Acc: 54.6% | Test Acc: 56.7%\n",
      "Epoch [200/200] Loss: 0.7507 | Train Acc: 54.0% | Test Acc: 56.2%\n",
      "\n",
      "[최종 테스트 정확도]: 56.22%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "df = pd.read_csv('kbo_mlp_training_data.csv')\n",
    "X = df.drop(columns=['game_id', 'target']).values\n",
    "y = df['target'].values.reshape(-1, 1)\n",
    "\n",
    "# 데이터 분할 (셔플 없이 시간 순서대로)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 텐서 변환\n",
    "X_train_t = torch.FloatTensor(X_train_scaled)\n",
    "y_train_t = torch.FloatTensor(y_train)\n",
    "X_test_t = torch.FloatTensor(X_test_scaled)\n",
    "y_test_t = torch.FloatTensor(y_test)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=16, shuffle=True)\n",
    "\n",
    "# 2. 모델 설계 (Dropout 추가로 과적합 방지)\n",
    "class AdvancedKBOPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdvancedKBOPredictor, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(12, 8),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# 모델, 손실함수, 최적화기 설정\n",
    "model = AdvancedKBOPredictor()\n",
    "criterion = nn.BCELoss()\n",
    "# weight_decay는 가중치가 너무 커지지 않게 규제함 (L2 규제)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-2)\n",
    "\n",
    "# 3. 학습 루프\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 20회마다 평가 결과 출력\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_outputs = model(X_train_t)\n",
    "            train_acc = ((train_outputs > 0.5).float() == y_train_t).float().mean()\n",
    "            \n",
    "            test_outputs = model(X_test_t)\n",
    "            test_acc = ((test_outputs > 0.5).float() == y_test_t).float().mean()\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] Loss: {loss.item():.4f} | Train Acc: {train_acc:.1%} | Test Acc: {test_acc:.1%}\")\n",
    "\n",
    "# 4. 최종 결과 확인\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_pred = (model(X_test_t) > 0.5).float()\n",
    "    print(f\"\\n[최종 테스트 정확도]: {accuracy_score(y_test, final_pred):.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
